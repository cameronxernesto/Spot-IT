# -*- coding: utf-8 -*-
"""Lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dj6rd1hKeQrNCCsA46ylofwj1yr7xko5
"""

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="vOQ5582UEj5o9sfljRjK")
project = rf.workspace("jason-rr0iu").project("merged-m3nts")
version = project.version(1)
dataset = version.download("yolov11")

!pip install ultralytics
#  albumentations
from ultralytics import YOLO

# Load a model
model = YOLO("yolo11s.yaml")  # build a new model from YAML
model = YOLO("yolo11s.pt")  # load a pretrained model (recommended for training)
model = YOLO("yolo11s.yaml").load("yolo11s.pt")  # build from YAML and transfer weights

# Train the model
results = model.train(data="/content/Merged-1/data.yaml",
                      project="content/drive/MyDrive/CSE185/Lab4",
                      name = "spotit",
                      epochs=100, imgsz=640)

# @software{yolo11_ultralytics,
#   author = {Glenn Jocher and Jing Qiu},
#   title = {Ultralytics YOLO11},
#   version = {11.0.0},
#   year = {2024},
#   url = {https://github.com/ultralytics/ultralytics},
#   orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
#   license = {AGPL-3.0}
# }

from google.colab import drive
drive.mount('/content/drive')

from ultralytics import YOLO
import os
import csv
from collections import defaultdict

evaluation_folder = '/content/drive/MyDrive/CSE185/Lab4/eval_set'
model_path = '/content/best.pt'
output_csv = 'evaluation_results.csv'

model = YOLO(model_path)
rows = [("ID", "Class")]

for filename in os.listdir(evaluation_folder):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        image_path = os.path.join(evaluation_folder, filename)

        conf = 0.8


        while conf >= 0.1:
            results = model(image_path, conf=conf, iou=0.3, verbose=False)
            boxes = results[0].boxes

            if boxes is not None and len(boxes) > 0:
                class_confidences = defaultdict(list)

                # Collect confidence scores per class
                for box in boxes:
                    class_idx = int(box.cls[0].item())
                    class_name = model.names[class_idx]
                    confidence_score = float(box.conf[0].item())
                    class_confidences[class_name].append(confidence_score)

                # Keep classes with at least 2 detections
                duplicate_classes = {
                    cls: confs for cls, confs in class_confidences.items() if len(confs) >= 2
                }

                if duplicate_classes:
                    # Choose class with highest average confidence
                    most_confident_class = max(
                        duplicate_classes.items(),
                        key=lambda item: sum(item[1]) / len(item[1])
                    )[0]
                    detected_class = most_confident_class
                    break
                else:
                    conf -= 0.1
            else:
                conf -= 0.1

        rows.append((filename, detected_class))

# Write results to CSV
with open(output_csv, mode="w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(rows)

!pip install roboflow supervision
from roboflow import Roboflow
import supervision as sv
import cv2
import os
import csv
from collections import Counter, defaultdict


# Setup
evaluation_folder = 'drive/MyDrive/CSE185/Lab4/eval_set'
output_csv = 'evaluation_results.csv'

rf = Roboflow(api_key="vOQ5582UEj5o9sfljRjK")
project = rf.workspace().project("spot-it-2")
model = project.version(6).model

rows = [("ID", "Class")]

for filename in os.listdir(evaluation_folder):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        image_path = os.path.join(evaluation_folder, filename)
        confidence = 80
        while confidence >= 10:
            result = model.predict(image_path, confidence=confidence, overlap=30).json()
            predictions = result["predictions"]


            if result["predictions"]:
                class_confidences = defaultdict(list)

                # Count and collect confidences for each class
                for pred in result["predictions"]:
                    class_name = pred["class"]
                    pred_conf = pred["confidence"]
                    class_confidences[class_name].append(pred_conf)

                # Filter classes with at least 2 detections
                duplicate_classes = {
                    cls: confs for cls, confs in class_confidences.items() if len(confs) >= 2
                }

                if duplicate_classes:
                    # Pick the class with the highest average confidence
                    most_confident_class = max(
                        duplicate_classes.items(),
                        key=lambda item: sum(item[1]) / len(item[1])
                    )[0]

                    rows.append((filename, most_confident_class))
                    break

                else:
                  confidence -= 10
            else:
                confidence -=10



# Write results to CSV
with open(output_csv, mode="w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(rows)